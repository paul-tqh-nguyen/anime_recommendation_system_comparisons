<!DOCTYPE html>
<html>
  <head>
    <title>Collaborative Filtering For Anime</title>
    <link rel="stylesheet" type="text/css" href="./index.css">
    <script src="https://d3js.org/d3.v5.js"></script>
  </head>
  <body>
  <body>
    <header class="stone-background">
      <div class="vertical-padding">
	<h1 style="">Collaborative Filtering For Anime</h1>
	<p>A comparison of matrix factorization against a neural recommender system.</p>
      </div>
    </header>
    <section id="introduction">
      <div class="horizontal-padding vertical-padding">
	<h3>Intro</h3>
	<p>In this post, we'll compare an anime recommender system using matrix factorization against a neural recommender system.</p>
	<p>We'll be comparing them on an <a target="_blank" href="https://www.kaggle.com/CooperUnion/anime-recommendations-database?select=rating.csv">anime review dataset</a>. The dataset contains 7.8 million reviews on a scale of 1 to 10 from 73.5 thousand users on 11.2 thousand animes.</p>
      </div>
    </section>
    <section id="experiment-overview" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Overview</h3>
	<p>We'll first go over our two models and then discuss how we will compare them.</p>
	<p>Our first model uses <a target="_blank" href="https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)#:~:text=Matrix%20factorization%20is%20a%20class,two%20lower%20dimensionality%20rectangular%20matrices.">matrix factorization</a>. A particularly good explanation of matrix factorization can be found <a target="_blank" href="https://www.youtube.com/watch?v=giIXNoiqO_U&list=PL-6SiIrhTAi6x4Oq28s7yy94ubLzVXabj">in this series</a> (though the term matrix factorization is not frequently used, the method taught is a form of Funk matrix factorization). Our matrix factorization model is a Funk matrix factorization model. We used L2 regularization in our matrix factorization model on both the user representation vectors and the anime representation vectors. Dropout was applied to each representation vector prior to calculating their dot product (which would be the predicted score the user gave the anime). Be sure to read the next section regarding our experiment results for further details about our choice to use dropout.</p>
	<p>Below is a depiction of our neural recommender system:</p>
	<div id="nn-depiction-container" class="svg-container-center"><svg></svg></div>
	<p>In our neural model, we first embed every user and anime. We then concatenate the two embeddings and put the concatenation through several dense linear layers with a ReLU activation. We take a linear combination of the result from the dense linear layers by passing it through a fully connected layer. The result from the fully connected layer is then squashed through a sigmoid and scaled by 10 to predict the user's score for the anime.</p>
	<p>Our neural model used L2 regularization on the user and anime embeddings. Dropout was applied to all embedding and dense layers.</p>
	<p>Note that the use of embedding layers in our neural model is very similar to matrix factorization since embedding layers are frequently implemented as matrices.</p>
	<p>The embeddings for the users and animes are <em>not</em> normalized in either model.</p>
	<p>Note that both models are transductive. They cannot predict ratings for users or animes not present during training. This influences how we create our test set to evaluate our models.</p>
	<p>We can't simply split our data by reviews since there might be users in the test set that haven't been seen before during training. Neither model will have any hope of returning reasonable estimates in this case.</p>
	<p>Thus, we created our training/validation/testing split by taking each user's reviews, putting 65% of them in the training set, 15% of them in the validation set, and 20% of them into the testing set. Each split will have some reviews for each user.</p>
	<p>We discarded any users who had fewer than 100 reviews and any animes that had fewer than 100 reviews.</p>
	<p>We optimized both models using <a target="_blank" href="https://arxiv.org/abs/1711.05101">adaptive momentum with weight decay</a> along with a linear learning rate schedule. We used mean squared error as our loss function. Be sure to read the next section regarding our results for further details about our optimizer choice.</p>
	<p>We conducted hyperparameter search for both models using a <a target="_blank" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">tree-structured Parzen estimator</a>.</p>
	<p style="padding-bottom: 0px;">The hyperparameters for our matrix factorization model were:</p>
	<ul>
	  <li>Initial Learning Rate</li>
	  <li>Number of Training Epochs</li>
	  <li>Batch Size</li>
	  <li>Max Gradient Clipping Threshold</li>
	  <li>User & Anime Vector Representation Size</li>
	  <li>L2 Regularization Multiplier</li>
	  <li>Dropout Probability</li>
	</ul>
	<p style="padding-bottom: 0px;">The hyperparameters for our neural model were:</p>
	<ul>
	  <li>Initial Learning Rate</li>
	  <li>Number of Training Epochs</li>
	  <li>Batch Size</li>
	  <li>Max Gradient Clipping Threshold</li>
	  <li>User & Anime Vector Representation Size</li>
	  <li>Number of Dense Linear Layers</li>
	  <li>L2 Regularization Multiplier</li>
	  <li>Dropout Probability</li>
	</ul>
      </div>
    </section>
    <section id="experiment-results">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Results</h3>
	<p>Below are the results for the best models we were able to find so far from our hyperparameter search. The models are ranked based on validation score.</p>
	<p>Click on the model labels below to see the results.</p>
	<p>Hover over the bars in the bar charts or points in the scatter plots for more details about the data shown.</p>
	<p>Matrix Factorization Results:</p>
	<div id="linear-result"><p>Loading results...</p></div>
	<p>Neural Model Results:</p>
	<div id="deep-result"><p>Loading results...</p></div>
	<p>Looking at our results, we'll see that both models perform very similarly with respect to validation and testing losses.</p>
	<p>The first thing to notice is that our validation scores are consistently better than our testing scores. This might be a sign of overfitting. We're currently investigating this.</p>
	<p>Early on, we suspected that there might be problems due to our data processing where the models may perform better with users who have more reviews than those with fewer reviews. If we look at our User Mean MSE Loss vs User Example Count scatter plots, we'll notice that there are users with the highest mean loss tend to be the ones with lower example counts. Ideally, our models generally would not get a higher loss on users with low example counts. If they do, then we'd expect that there would be many users with low example counts who would get high losses. Looking at the User Count vs MSE Loss Histograms for our models, most of the users have a low MSE loss. Given that this is the case, it seems that the users with high losses are outliers.</p>
	<p>Looking at our matrix factorization models, we'll notice that our best performing models have a close-to-zero dropout probability. This makes sense since <a target="_blank" href="https://nlp.stanford.edu/pubs/wager2013dropout.pdf">dropout for generalized linear models is equivalent to adaptive L2 regularization</a>. It seems that adaptive regularization didn't yield much benefit over uniform L2 regularization for this dataset. If we were to repeat this experiment in the future or try matrix factorization on a different dataset, we would reconsider the need for dropout when we're applying L2 regularization.</p>
	<p>The choice to use L2 regularization might've been redundant or have had adverse affects on our gradient descent search since we used adaptive momentum with weight decay given that <a target="_blank" href="https://arxiv.org/pdf/1711.05101.pdf">L2 regularization is not effective in Adam</a>. Our choice of optimizer and loss function might be different for future experiments given this information.</p>
      </div>
    </section>
    <section id="conclusion" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Conclusion</h3>
	<p>Our matrix factorization models and neural models achieve similar performance, so it might seem that both architectures perform similarly. However, looking at our results so far, there's still some amount of work to be done (in particular with respect to further exploration with optimizer & regularization choices). Thus, despite the fact that our initial findings show that our two architectures perform similarly, there's still further exploration to do before we can conclusively make that claim.</p>
      </div>
      <table style="table-layout: fixed; width: 100%; padding-top: 40px; padding-bottom: 40px;">
	<tr>
	  <td style="width:10%;"></td>
	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://github.com/paul-tqh-nguyen">
      		<div class="card-text">
      		  <p>Interested in my work?</p>
      		  <p><b>See my projects on GitHub.</b></p>
      		</div>
      	      </a>
      	    </card>
	  </td>
	  <td style="width:20%;"></td>
	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://paul-tqh-nguyen.github.io/about/">
      		<div class="card-text">
      		  <p>Want to learn more about me?</p>
      		  <p><b>Visit my website.</b></p>
      		</div>
      	      </a>
      	    </card>
	  </td>
	  <td style="width:10%;"></td>
	</tr>
      </table>
    </section>
    <script src="index.js"></script>
  </body>
</html>
